cesga/2020 unloaded
        Preparing the environment for use of the CESGA 2025 software stack.
        Please rebuild modules cache: module --ignore-cache avail
     
cesga/2025 loaded

The following have been reloaded with a version change:
  1) cesga/2020 => cesga/2025

2025-10-25 19:52:50.269257: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-25 19:52:50.319116: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-10-25 19:52:52.320626: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Parameter 'function'=<bound method SQuADTrainer.preprocess_function of <__main__.SQuADTrainer object at 0x150789bb1f50>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  10%|█         | 1000/10000 [00:00<00:03, 2842.72 examples/s]Map:  20%|██        | 2000/10000 [00:00<00:02, 3387.84 examples/s]Map:  30%|███       | 3000/10000 [00:00<00:01, 3701.73 examples/s]Map:  40%|████      | 4000/10000 [00:01<00:01, 3902.71 examples/s]Map:  50%|█████     | 5000/10000 [00:01<00:01, 3979.10 examples/s]Map:  60%|██████    | 6000/10000 [00:01<00:01, 3169.35 examples/s]Map:  70%|███████   | 7000/10000 [00:02<00:00, 3437.60 examples/s]Map:  80%|████████  | 8000/10000 [00:02<00:00, 3665.99 examples/s]Map:  90%|█████████ | 9000/10000 [00:02<00:00, 3729.34 examples/s]Map: 100%|██████████| 10000/10000 [00:02<00:00, 3875.89 examples/s]Map: 100%|██████████| 10000/10000 [00:02<00:00, 3578.44 examples/s]
[W1025 19:53:18.729245898 CPUAllocator.cpp:249] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  10%|█         | 1000/10000 [00:00<00:02, 3686.09 examples/s]Map:  20%|██        | 2000/10000 [00:00<00:02, 3822.47 examples/s]Map:  30%|███       | 3000/10000 [00:00<00:01, 3910.30 examples/s]Map:  40%|████      | 4000/10000 [00:01<00:01, 3966.69 examples/s]Map:  50%|█████     | 5000/10000 [00:01<00:01, 3975.97 examples/s]Map:  60%|██████    | 6000/10000 [00:01<00:01, 3995.81 examples/s]Map:  70%|███████   | 7000/10000 [00:01<00:00, 3996.82 examples/s]Map:  80%|████████  | 8000/10000 [00:02<00:00, 4042.83 examples/s]Map:  90%|█████████ | 9000/10000 [00:02<00:00, 3950.24 examples/s]Map: 100%|██████████| 10000/10000 [00:02<00:00, 3988.46 examples/s]Map: 100%|██████████| 10000/10000 [00:02<00:00, 3897.17 examples/s]
